{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Actor_Critic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvOfP8bXD62j3sVvO2tNlx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLY2BFyb_tto"
      },
      "source": [
        "## This code implements an Actor-Critic Framework for the Cart Pole example provided by Open AI Gym.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXAu5BN0AUJR"
      },
      "source": [
        "The code is based on the Pytorch Examples (https://github.com/pytorch/examples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLr4WxjIlRV"
      },
      "source": [
        "# Loading Libraries\n",
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcrXWZfiQHs9",
        "outputId": "3c2517fa-86c3-4d0f-c416-b8f1c212edd2"
      },
      "source": [
        "seed=1234\n",
        "# Making the cartpole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f43f3971510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQF4I2FvQPwd"
      },
      "source": [
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAxWmSU_QTJx"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.layer1 = nn.Linear(4, 128)\n",
        "        self.action_head = nn.Linear(128, 2)\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        action_scores = self.action_head(x)\n",
        "        state_values = self.value_head(x)\n",
        "        return F.softmax(action_scores, dim=-1), state_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpcRF66zQcLO"
      },
      "source": [
        "model = Policy()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgTsVY6TQirW"
      },
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs, state_value = model(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "    return action.item()\n",
        "\n",
        "\n",
        "def finish_episode():\n",
        "    R = 0\n",
        "    gamma=0.99\n",
        "    saved_actions = model.saved_actions\n",
        "    policy_losses = []\n",
        "    value_losses = []\n",
        "    rewards = []\n",
        "    for r in model.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        rewards.insert(0, R)\n",
        "    rewards = torch.tensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
        "    for (log_prob, value), r in zip(saved_actions, rewards):\n",
        "        reward = r - value.item()\n",
        "        policy_losses.append(-log_prob * reward)\n",
        "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))\n",
        "    optimizer.zero_grad()\n",
        "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1n3uLWFR_cf",
        "outputId": "fd4cb458-c27b-4ab0-9cca-e5c5b68d88f1"
      },
      "source": [
        "render=False\n",
        "running_reward = 10\n",
        "log_interval=100\n",
        "for i_episode in range(1,1001):\n",
        "    state = env.reset()\n",
        "    for t in range(10000): \n",
        "        action = select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if render:\n",
        "            env.render()\n",
        "        model.rewards.append(reward)\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "    running_reward = running_reward * 0.99 + t * 0.01\n",
        "    finish_episode()\n",
        "    if i_episode % log_interval == 0:\n",
        "        print('Episode {}\\t Time steps: {:5d} \\t Running Reward: {:.2f}'.format(i_episode,t, running_reward))\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 100\t Time steps:   135 \t Running Reward: 83.58\n",
            "Episode 200\t Time steps:   466 \t Running Reward: 138.76\n",
            "Episode 300\t Time steps:   499 \t Running Reward: 337.70\n",
            "Episode 400\t Time steps:   438 \t Running Reward: 411.42\n",
            "Episode 500\t Time steps:   499 \t Running Reward: 452.34\n",
            "Episode 600\t Time steps:   499 \t Running Reward: 472.21\n",
            "Episode 700\t Time steps:   499 \t Running Reward: 489.19\n",
            "Solved! Running reward is now 489.19224801196765 and the last episode runs to 499 time steps!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}