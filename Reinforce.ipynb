{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforce.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyJfg48W5+fGZJO1c0DPJ6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeSJkVi7AipC"
      },
      "source": [
        "## This code implements the REINFORCE algorithm for the Cart Pole example provided by Open AI Gym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ1hN1RIAwkA"
      },
      "source": [
        "This code is based on the Pytorch examples (https://github.com/pytorch/examples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uPrNxVv3rZH"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSAIz8p033V2",
        "outputId": "7b140afa-9bd9-465c-bbf4-ba655fba21e8"
      },
      "source": [
        "seed = 1234\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbeb6d754f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERph1azp4BfV"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.input_layer = nn.Linear(4, 128)\n",
        "        self.output = nn.Linear(128, 2)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input_layer(x))\n",
        "        action_scores = self.output(x)\n",
        "        return F.softmax(action_scores, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-Vq7VgH40B0"
      },
      "source": [
        "policy = Policy()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjH_ORFT403l"
      },
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    probs = policy(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    policy.saved_log_probs.append(m.log_prob(action))\n",
        "    return action.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcsAqrkG45Uq"
      },
      "source": [
        "def finish_episode():\n",
        "    R = 0\n",
        "    gamma=0.99\n",
        "    policy_loss = []\n",
        "    rewards = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        rewards.insert(0, R)\n",
        "    rewards = torch.tensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
        "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kno7PhHz49z5",
        "outputId": "3229d339-985b-4dd0-925e-5f2a7e112974"
      },
      "source": [
        "running_reward = 10\n",
        "render=False\n",
        "log_interval=10\n",
        "for i_episode in count(1):\n",
        "    state = env.reset()\n",
        "    for t in range(10000):  # Don't infinite loop while learning\n",
        "        action = select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        if render:\n",
        "            env.render()\n",
        "        policy.rewards.append(reward)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    running_reward = running_reward * 0.99 + t * 0.01\n",
        "    finish_episode()\n",
        "    if i_episode % log_interval == 0:\n",
        "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
        "            i_episode, t, running_reward))\n",
        "    if running_reward > env.spec.reward_threshold:\n",
        "        print(\"Solved! Running reward is now {} and \"\n",
        "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 10\tLast length:     9\tAverage length: 10.38\n",
            "Episode 20\tLast length:    19\tAverage length: 10.85\n",
            "Episode 30\tLast length:    46\tAverage length: 13.77\n",
            "Episode 40\tLast length:     9\tAverage length: 15.31\n",
            "Episode 50\tLast length:   166\tAverage length: 20.69\n",
            "Episode 60\tLast length:    54\tAverage length: 28.59\n",
            "Episode 70\tLast length:   124\tAverage length: 34.04\n",
            "Episode 80\tLast length:    72\tAverage length: 40.26\n",
            "Episode 90\tLast length:    65\tAverage length: 45.72\n",
            "Episode 100\tLast length:    40\tAverage length: 47.37\n",
            "Episode 110\tLast length:    72\tAverage length: 47.15\n",
            "Episode 120\tLast length:    52\tAverage length: 48.14\n",
            "Episode 130\tLast length:   128\tAverage length: 55.04\n",
            "Episode 140\tLast length:   410\tAverage length: 73.21\n",
            "Episode 150\tLast length:   105\tAverage length: 93.85\n",
            "Episode 160\tLast length:    15\tAverage length: 87.71\n",
            "Episode 170\tLast length:    30\tAverage length: 81.45\n",
            "Episode 180\tLast length:    15\tAverage length: 75.62\n",
            "Episode 190\tLast length:    21\tAverage length: 70.66\n",
            "Episode 200\tLast length:    26\tAverage length: 66.58\n",
            "Episode 210\tLast length:    32\tAverage length: 62.90\n",
            "Episode 220\tLast length:    54\tAverage length: 60.45\n",
            "Episode 230\tLast length:    40\tAverage length: 58.65\n",
            "Episode 240\tLast length:    18\tAverage length: 55.49\n",
            "Episode 250\tLast length:    25\tAverage length: 52.63\n",
            "Episode 260\tLast length:    16\tAverage length: 49.45\n",
            "Episode 270\tLast length:    23\tAverage length: 46.78\n",
            "Episode 280\tLast length:    26\tAverage length: 44.78\n",
            "Episode 290\tLast length:    36\tAverage length: 43.77\n",
            "Episode 300\tLast length:    73\tAverage length: 44.39\n",
            "Episode 310\tLast length:   161\tAverage length: 51.13\n",
            "Episode 320\tLast length:   119\tAverage length: 59.47\n",
            "Episode 330\tLast length:   124\tAverage length: 65.96\n",
            "Episode 340\tLast length:   280\tAverage length: 76.44\n",
            "Episode 350\tLast length:   499\tAverage length: 114.56\n",
            "Episode 360\tLast length:   181\tAverage length: 133.03\n",
            "Episode 370\tLast length:   164\tAverage length: 136.52\n",
            "Episode 380\tLast length:   175\tAverage length: 139.64\n",
            "Episode 390\tLast length:   235\tAverage length: 144.75\n",
            "Episode 400\tLast length:   499\tAverage length: 168.87\n",
            "Episode 410\tLast length:   499\tAverage length: 200.44\n",
            "Episode 420\tLast length:   499\tAverage length: 228.99\n",
            "Episode 430\tLast length:   499\tAverage length: 254.81\n",
            "Episode 440\tLast length:   499\tAverage length: 278.15\n",
            "Episode 450\tLast length:    89\tAverage length: 286.38\n",
            "Episode 460\tLast length:   499\tAverage length: 277.25\n",
            "Episode 470\tLast length:   206\tAverage length: 271.54\n",
            "Episode 480\tLast length:   499\tAverage length: 277.84\n",
            "Episode 490\tLast length:   221\tAverage length: 278.43\n",
            "Episode 500\tLast length:   245\tAverage length: 273.29\n",
            "Episode 510\tLast length:   499\tAverage length: 284.17\n",
            "Episode 520\tLast length:   499\tAverage length: 304.71\n",
            "Episode 530\tLast length:   499\tAverage length: 323.29\n",
            "Episode 540\tLast length:   499\tAverage length: 340.09\n",
            "Episode 550\tLast length:   499\tAverage length: 355.29\n",
            "Episode 560\tLast length:   499\tAverage length: 369.03\n",
            "Episode 570\tLast length:   499\tAverage length: 381.46\n",
            "Episode 580\tLast length:   499\tAverage length: 392.70\n",
            "Episode 590\tLast length:   499\tAverage length: 402.86\n",
            "Episode 600\tLast length:   499\tAverage length: 409.86\n",
            "Episode 610\tLast length:   499\tAverage length: 418.38\n",
            "Episode 620\tLast length:   499\tAverage length: 426.09\n",
            "Episode 630\tLast length:   499\tAverage length: 433.06\n",
            "Episode 640\tLast length:   499\tAverage length: 439.37\n",
            "Episode 650\tLast length:   499\tAverage length: 445.07\n",
            "Episode 660\tLast length:   499\tAverage length: 450.23\n",
            "Episode 670\tLast length:   499\tAverage length: 454.89\n",
            "Episode 680\tLast length:   499\tAverage length: 459.11\n",
            "Episode 690\tLast length:   499\tAverage length: 462.92\n",
            "Episode 700\tLast length:   499\tAverage length: 466.37\n",
            "Episode 710\tLast length:   499\tAverage length: 469.49\n",
            "Episode 720\tLast length:   499\tAverage length: 472.31\n",
            "Episode 730\tLast length:   499\tAverage length: 474.86\n",
            "Solved! Running reward is now 475.1062756944753 and the last episode runs to 499 time steps!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}